#!/usr/bin/env python3
"""
SAM3 Interactive Vision Studio
åŸºäº SAM3 çš„äº¤äº’å¼å›¾åƒåˆ†å‰²ä¸è§†é¢‘è·Ÿè¸ªç³»ç»Ÿ
"""

import os
import sys
import time
import io
import numpy as np
import torch
import gradio as gr
from PIL import Image
import cv2
from pathlib import Path
import tempfile
import json

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥sam3æ¨¡å—
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# å¯¼å…¥SAM3ç›¸å…³æ¨¡å—
try:
    from sam3.model_builder import build_sam3_image_model, build_sam3_video_model
    from sam3.model.sam3_image_processor import Sam3Processor
    from sam3.model.sam3_video_predictor import Sam3VideoPredictor
    from sam3.model.data_misc import FindStage
    from sam3.visualization_utils import plot_results, visualize_formatted_frame_output, render_masklet_frame
    from sam3.model import box_ops
except ImportError as e:
    print(f"å¯¼å…¥SAM3æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…SAM3ä¾èµ–")
    sys.exit(1)

# å…¨å±€å˜é‡
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# åˆå§‹åŒ–æ¨¡å‹
def initialize_models():
    """åˆå§‹åŒ–SAM3å›¾åƒå’Œè§†é¢‘é¢„æµ‹å™¨"""
    try:
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_dir = current_dir / "models"
        checkpoint_path = model_dir / "sam3.pt"
        bpe_path = current_dir / "assets" / "bpe_simple_vocab_16e6.txt.gz"
        
        if not checkpoint_path.exists():
            print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            print("è¯·ä¸‹è½½SAM3æ¨¡å‹æ–‡ä»¶åˆ°ç›®å½•")
            return None, None
            
        if not bpe_path.exists():
            print(f"BPEæ–‡ä»¶ä¸å­˜åœ¨: {bpe_path}")
            return None, None
            
        # åˆå§‹åŒ–å›¾åƒæ¨¡å‹
        image_model = build_sam3_image_model(
            checkpoint_path=str(checkpoint_path),
            bpe_path=str(bpe_path),
            device=DEVICE
        )
        
        # åˆ›å»ºå›¾åƒå¤„ç†å™¨
        image_predictor = Sam3Processor(image_model, device=DEVICE)
        
        # åˆå§‹åŒ–è§†é¢‘é¢„æµ‹å™¨
        video_predictor = Sam3VideoPredictor(
            checkpoint_path=str(checkpoint_path),
            bpe_path=str(bpe_path)
        )
        
        print("æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        return image_predictor, video_predictor
        
    except Exception as e:
        print(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return None, None

# å…¨å±€é¢„æµ‹å™¨å®ä¾‹
image_predictor, video_predictor = initialize_models()

def handle_image_click(img, original_img, evt: gr.SelectData, mode, current_points, current_boxes, click_state):
    """å¤„ç†å›¾åƒç‚¹å‡»äº‹ä»¶ï¼Œæä¾›å®æ—¶è§†è§‰åé¦ˆ"""
    if img is None:
        return img, current_points, current_boxes, click_state, "è¯·å…ˆä¸Šä¼ å›¾åƒ"
    
    # å¦‚æœæ²¡æœ‰åŸå§‹å›¾åƒï¼Œå°±ä½¿ç”¨å½“å‰å›¾åƒä½œä¸ºåŸå§‹å›¾åƒ
    if original_img is None:
        original_img = img.copy()
        
    # åŸºäºå½“å‰æ˜¾ç¤ºçš„å›¾åƒè¿›è¡Œç»˜åˆ¶
    vis_img = img.copy()
    
    x, y = evt.index
    x, y = int(x), int(y)
    
    info_msg = ""
    
    if mode == "ğŸ“ ç‚¹æç¤º (Point)":
        new_point = f"{x},{y}"
        if current_points:
            current_points += f";{new_point}"
        else:
            current_points = new_point
            
        # åœ¨å›¾åƒä¸Šç”»çº¢è‰²åœ†ç‚¹
        cv2.circle(vis_img, (x, y), 6, (255, 0, 0), -1) # çº¢è‰²å®å¿ƒåœ†
        cv2.circle(vis_img, (x, y), 6, (255, 255, 255), 1) # ç™½è‰²æè¾¹
        
        info_msg = f"âœ… å·²æ·»åŠ ç‚¹: {new_point}"
        return vis_img, current_points, current_boxes, None, info_msg
        
    elif mode == "ğŸ”² æ¡†æç¤º (Box)":
        if click_state is None:
            # ç¬¬ä¸€æ¬¡ç‚¹å‡» - ç”»èµ·ç‚¹ï¼ˆè“è‰²ï¼‰
            click_state = [x, y]
            cv2.circle(vis_img, (x, y), 6, (0, 0, 255), -1) # è“è‰²å®å¿ƒåœ†
            cv2.circle(vis_img, (x, y), 6, (255, 255, 255), 1) # ç™½è‰²æè¾¹
            info_msg = f"ğŸ”µ å·²è®°å½•èµ·ç‚¹: {x},{y}ï¼Œè¯·ç‚¹å‡»å¯¹è§’ç‚¹å®Œæˆæ¡†é€‰"
            return vis_img, current_points, current_boxes, click_state, info_msg
        else:
            # ç¬¬äºŒæ¬¡ç‚¹å‡» - ç”»æ¡†ï¼ˆç»¿è‰²ï¼‰
            x1, y1 = click_state
            x2, y2 = x, y
            
            xmin = min(x1, x2)
            ymin = min(y1, y2)
            xmax = max(x1, x2)
            ymax = max(y1, y2)
            
            # ç¡®ä¿æ¡†æœ‰å¤§å°
            if xmin == xmax: xmax += 1
            if ymin == ymax: ymax += 1
            
            new_box = f"{xmin},{ymin},{xmax},{ymax}"
            if current_boxes:
                current_boxes += f";{new_box}"
            else:
                current_boxes = new_box
            
            # ç”»ç»¿è‰²çŸ©å½¢æ¡†
            cv2.rectangle(vis_img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 3)
                
            info_msg = f"âœ… å·²æ·»åŠ æ¡†: {new_box}"
            return vis_img, current_points, current_boxes, None, info_msg
    
    return vis_img, current_points, current_boxes, click_state, info_msg

def segment_image(
    input_image,
    text_prompt,
    confidence_threshold,
    point_prompt,
    box_prompt,
    original_image=None,
    progress=gr.Progress()
):
    """å›¾åƒåˆ†å‰²åŠŸèƒ½"""
    # ä¼˜å…ˆä½¿ç”¨åŸå§‹å›¾åƒï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨è¾“å…¥å›¾åƒ
    image_to_process = original_image if original_image is not None else input_image
    
    if image_to_process is None:
        return None, "è¯·ä¸Šä¼ å›¾åƒ"
        
    if not text_prompt and not point_prompt and not box_prompt:
        return None, "è¯·æä¾›è‡³å°‘ä¸€ç§æç¤ºï¼ˆæ–‡æœ¬ã€ç‚¹æˆ–æ¡†ï¼‰"
    
    try:
        if image_predictor is None:
            return None, "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"
            
        start_time = time.time()
        progress(0.1, desc="æ­£åœ¨åŠ è½½å›¾åƒ...")
        
        # è½¬æ¢å›¾åƒæ ¼å¼
        if isinstance(image_to_process, np.ndarray):
            image = Image.fromarray(image_to_process)
        else:
            image = image_to_process
            
        # è®¾ç½®å›¾åƒ
        state = image_predictor.set_image(image)
        progress(0.3, desc="è§£ææç¤ºä¿¡æ¯...")
        
        # å¤„ç†æ–‡æœ¬æç¤º
        if text_prompt:
            state = image_predictor.set_text_prompt(text_prompt, state)
            
        # å¤„ç†ç‚¹æç¤º
        if point_prompt:
            points = []
            for point_str in point_prompt.split(';'):
                if point_str:
                    try:
                        x, y = map(float, point_str.split(','))
                        points.append([x, y])
                    except ValueError:
                        continue
                        
            if points:
                width, height = image.size
                normalized_points = []
                for x, y in points:
                    normalized_points.append([x/width, y/height])
                    
                for point in normalized_points:
                    box_size = min(width, height) * 0.05
                    box_width = box_size / width
                    box_height = box_size / height
                    box = [point[0], point[1], box_width, box_height]
                    state = image_predictor.add_geometric_prompt(box, True, state)
        
        # å¤„ç†æ¡†æç¤º
        if box_prompt:
            boxes = []
            for box_str in box_prompt.split(';'):
                if box_str:
                    try:
                        x1, y1, x2, y2 = map(float, box_str.split(','))
                        boxes.append([x1, y1, x2, y2])
                    except ValueError:
                        continue
                        
            if boxes:
                width, height = image.size
                normalized_boxes = []
                for x1, y1, x2, y2 in boxes:
                    center_x = (x1 + x2) / 2 / width
                    center_y = (y1 + y2) / 2 / height
                    box_width = (x2 - x1) / width
                    box_height = (y2 - y1) / height
                    normalized_boxes.append([center_x, center_y, box_width, box_height])
                    
                for box in normalized_boxes:
                    state = image_predictor.add_geometric_prompt(box, True, state)
        
        # è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼
        state = image_predictor.set_confidence_threshold(confidence_threshold, state)
        
        progress(0.7, desc="æ¨¡å‹æ¨ç†ä¸­...")
        
        # è·å–ç»“æœ
        if "boxes" in state and len(state["boxes"]) > 0:
            # å¯è§†åŒ–ç»“æœ
            import matplotlib.pyplot as plt
            
            # ä½¿ç”¨å®˜æ–¹çš„ plot_results æ¥å£è¿›è¡Œç»˜åˆ¶
            # plot_results å†…éƒ¨ä¼šåˆ›å»º figure å¹¶ç»˜åˆ¶ masks, boxes, scores
            # æ³¨æ„ï¼šå®ƒä¼šæ‰“å°æ‰¾åˆ°çš„å¯¹è±¡æ•°é‡ï¼Œä½†è¿™ä¸å½±å“ Gradio æ˜¾ç¤º
            plot_results(image, state)
            
            # è·å–å½“å‰çš„ figure (ç”± plot_results åˆ›å»º) å¹¶è½¬æ¢ä¸º PIL å›¾åƒ
            buf = io.BytesIO()
            plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
            buf.seek(0)
            result_image = Image.open(buf)
            plt.close() # å…³é—­ figure é‡Šæ”¾å†…å­˜
            
            processing_time = time.time() - start_time
            info = f"âœ¨ å¤„ç†å®Œæˆ | è€—æ—¶: {processing_time:.2f}s | æ£€æµ‹åˆ° {len(state['boxes'])} ä¸ªç›®æ ‡"
            
            return result_image, info
        else:
            return image, "âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•å¯¹è±¡ï¼Œè¯·å°è¯•è°ƒæ•´æç¤ºæˆ–é™ä½ç½®ä¿¡åº¦é˜ˆå€¼"
            
    except Exception as e:
        return None, f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
def convert_output_format(outputs):
    """è½¬æ¢æ¨¡å‹è¾“å‡ºæ ¼å¼ä»¥é€‚é…å¯è§†åŒ–å‡½æ•°"""
    if not outputs: return {}
    
    # ç®€åŒ–ç‰ˆçš„è½¬æ¢é€»è¾‘ï¼Œå¤ç”¨ä¹‹å‰çš„æ ¸å¿ƒé€»è¾‘
    if "out_binary_masks" in outputs:
        formatted_outputs = {
            "out_boxes_xywh": [], "out_probs": [], "out_obj_ids": [], "out_binary_masks": []
        }
        
        masks = outputs["out_binary_masks"]
        if not isinstance(masks, (list, np.ndarray)): masks = [masks]
        formatted_outputs["out_binary_masks"] = list(masks)
        
        if "out_obj_ids" in outputs: formatted_outputs["out_obj_ids"] = list(outputs["out_obj_ids"])
        else: formatted_outputs["out_obj_ids"] = list(range(len(masks)))
            
        if "out_probs" in outputs: formatted_outputs["out_probs"] = list(outputs["out_probs"])
        else: formatted_outputs["out_probs"] = [1.0] * len(masks)

        if "out_boxes_xywh" in outputs:
            formatted_outputs["out_boxes_xywh"] = list(outputs["out_boxes_xywh"])
        else:
            # è®¡ç®—è¾¹ç•Œæ¡†
            for mask in formatted_outputs["out_binary_masks"]:
                if isinstance(mask, np.ndarray) and mask.any():
                    rows = np.any(mask, axis=1)
                    cols = np.any(mask, axis=0)
                    if rows.any() and cols.any():
                        y_min, y_max = np.where(rows)[0][[0, -1]]
                        x_min, x_max = np.where(cols)[0][[0, -1]]
                        h, w = mask.shape
                        formatted_outputs["out_boxes_xywh"].append([x_min/w, y_min/h, (x_max-x_min)/w, (y_max-y_min)/h])
                    else: formatted_outputs["out_boxes_xywh"].append([0,0,0,0])
                else: formatted_outputs["out_boxes_xywh"].append([0,0,0,0])
        return formatted_outputs
        
    # Fallback logic omitted for brevity as it mirrors previous implementation
    # ... (ä¿æŒä¹‹å‰çš„è¾…åŠ©é€»è¾‘)
    # è¿™é‡Œä¸ºäº†èŠ‚çœç©ºé—´ï¼Œæˆ‘ä»¬å‡è®¾ä¸»è¦è·¯å¾„èµ°é€šï¼Œå¦‚æœéœ€è¦å®Œæ•´fallbacké€»è¾‘å¯ä»¥å‚è€ƒä¸Šä¸€ç‰ˆä»£ç 
    # ä½†ä¸ºäº†ç¨³å¥æ€§ï¼Œè¿™é‡Œä¿ç•™åŸºæœ¬çš„æ©ç å¤„ç†
    elif "masks" in outputs:
         formatted_outputs = {"out_boxes_xywh": [], "out_probs": [], "out_obj_ids": [], "out_binary_masks": []}
         masks = outputs["masks"]
         # Handle list or tensor
         if not isinstance(masks, list) and hasattr(masks, 'shape'):
             if len(masks.shape) == 4: masks = [m[0] for m in masks.cpu().numpy()]
             elif len(masks.shape) == 3: masks = [m for m in masks.cpu().numpy()]
         
         for i, mask in enumerate(masks):
             if hasattr(mask, 'shape') and len(mask.shape) > 2: mask = mask.squeeze()
             formatted_outputs["out_binary_masks"].append(mask)
             formatted_outputs["out_obj_ids"].append(i)
             formatted_outputs["out_probs"].append(1.0)
             # ç®€å•boxè®¡ç®—
             if isinstance(mask, np.ndarray) and mask.any():
                 h, w = mask.shape
                 y, x = np.where(mask)
                 formatted_outputs["out_boxes_xywh"].append([x.min()/w, y.min()/h, (x.max()-x.min())/w, (y.max()-y.min())/h])
             else: formatted_outputs["out_boxes_xywh"].append([0,0,0,0])
         return formatted_outputs
         
    return {}

def process_video(
    input_video,
    text_prompt,
    confidence_threshold,
    progress=gr.Progress()
):
    """è§†é¢‘å¤„ç†åŠŸèƒ½"""
    if input_video is None:
        return None, "è¯·ä¸Šä¼ è§†é¢‘"
        
    if not text_prompt:
        return None, "è¯·æä¾›æ–‡æœ¬æç¤º"
    
    try:
        if video_predictor is None:
            return None, "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"
            
        start_time = time.time()
        progress(0.1, desc="æ­£åœ¨è§£æè§†é¢‘...")
        
        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            fd, output_path = tempfile.mkstemp(suffix=".mp4")
            os.close(fd)
            
            cap = cv2.VideoCapture(input_video)
            if not cap.isOpened(): return None, "æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶"
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            progress(0.2, desc="åˆå§‹åŒ–è·Ÿè¸ªä¼šè¯...")
            session_response = video_predictor.start_session(resource_path=input_video)
            session_id = session_response["session_id"]
            
            progress(0.3, desc="åº”ç”¨æç¤º...")
            video_predictor.add_prompt(session_id=session_id, frame_idx=0, text=text_prompt)
            
            progress(0.4, desc="æ­£åœ¨è·Ÿè¸ªç›®æ ‡...")
            outputs_per_frame = {}
            for response in video_predictor.handle_stream_request(
                request={"type": "propagate_in_video", "session_id": session_id}
            ):
                outputs_per_frame[response["frame_index"]] = response["outputs"]
            
            for frame_idx in range(frame_count):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if not ret: break
                    
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                if frame_idx in outputs_per_frame:
                    formatted_outputs = convert_output_format(outputs_per_frame[frame_idx])
                    if formatted_outputs.get("out_binary_masks"):
                        vis_frame = render_masklet_frame(
                            img=frame_rgb,
                            outputs=formatted_outputs,
                            frame_idx=frame_idx,
                            alpha=0.5
                        )
                    else: vis_frame = frame_rgb
                else: vis_frame = frame_rgb
                
                vis_frame_bgr = cv2.cvtColor(vis_frame, cv2.COLOR_RGB2BGR)
                out.write(vis_frame_bgr)
                
                progress_value = 0.4 + 0.5 * (frame_idx / frame_count)
                progress(progress_value, desc=f"æ¸²æŸ“å¸§ {frame_idx+1}/{frame_count}")
            
            cap.release()
            out.release()
            video_predictor.close_session(session_id)
            
            processing_time = time.time() - start_time
            info = f"âœ¨ å¤„ç†å®Œæˆ | è€—æ—¶: {processing_time:.2f}s | æ€»å¸§æ•°: {frame_count}"
            
            return str(output_path), info
            
    except Exception as e:
        return None, f"âŒ å¤„ç†å¤±è´¥: {str(e)}"

def create_demo():
    """åˆ›å»ºç¾åŒ–åçš„Gradioæ¼”ç¤ºç•Œé¢"""
    
    # è‡ªå®šä¹‰CSS
    custom_css = """
    .container { max-width: 1400px; width: 100%; margin: auto; padding-top: 20px; }
    h1 { text-align: center; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; color: #2d3748; margin-bottom: 10px; }
    .description { text-align: center; font-size: 1.1em; color: #4a5568; margin-bottom: 30px; }
    .gr-button-primary { background: linear-gradient(90deg, #4b6cb7 0%, #182848 100%); border: none; }
    .gr-box { border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
    #interaction-info { font-weight: bold; color: #2b6cb0; text-align: center; background-color: #ebf8ff; padding: 10px; border-radius: 5px; border: 1px solid #bee3f8; }
    
    /* è®©RadioæŒ‰é’®ç»„æ°´å¹³æ’‘æ»¡ */
    .mode-radio .wrap { display: flex; width: 100%; gap: 10px; }
    .mode-radio .wrap label { flex: 1; justify-content: center; text-align: center; }
    """

    theme = gr.themes.Soft(
        primary_hue="blue",
        secondary_hue="slate",
        font=[gr.themes.GoogleFont('Inter'), 'ui-sans-serif', 'system-ui', 'sans-serif']
    )

    with gr.Blocks(theme=theme, css=custom_css, title="SAM3 äº¤äº’å¼è§†è§‰å·¥ä½œå°") as demo:
        
        with gr.Column(elem_classes="container"):
            gr.Markdown("# ğŸ‘ï¸ SAM3 äº¤äº’å¼è§†è§‰å·¥ä½œå°")
            gr.Markdown("åŸºäº SAM3 çš„ä¸‹ä¸€ä»£å›¾åƒåˆ†å‰²ä¸è§†é¢‘è·Ÿè¸ªç³»ç»Ÿ", elem_classes="description")
            
            with gr.Tabs():
                # ================= å›¾åƒåˆ†å‰²æ ‡ç­¾é¡µ =================
                with gr.TabItem("ğŸ–¼ï¸ æ™ºèƒ½å›¾åƒåˆ†å‰²", id="tab_image"):
                    with gr.Row():
                        # å·¦ä¾§æ§åˆ¶æ 
                        with gr.Column(scale=5):
                            image_input = gr.Image(type="numpy", label="åŸå§‹å›¾åƒ (ç‚¹å‡»è¿›è¡Œäº¤äº’)", elem_id="input_image")
                            
                            # å­˜å‚¨åŸå§‹å›¾åƒçŠ¶æ€
                            original_image_state = gr.State(None)
                            click_state = gr.State(None) 
                            
                            with gr.Group():
                                gr.Markdown("### ğŸ® äº¤äº’æ¨¡å¼")
                                # ç¬¬ä¸€è¡Œï¼šæ¨¡å¼é€‰æ‹©
                                interaction_mode = gr.Radio(
                                    choices=["ğŸ“ ç‚¹æç¤º (Point)", "ğŸ”² æ¡†æç¤º (Box)"],
                                    value="ğŸ“ ç‚¹æç¤º (Point)",
                                    label="é€‰æ‹©æ¨¡å¼",
                                    show_label=False,
                                    elem_classes="mode-radio"
                                )
                                # ç¬¬äºŒè¡Œï¼šæ¸…ç©ºæŒ‰é’®ï¼ˆå…¨å®½ï¼‰
                                with gr.Row():
                                    clear_prompts_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæç¤º (Clear Prompts)", size="sm", variant="secondary")
                                
                                interaction_info = gr.Markdown("ğŸ‘† ç‚¹å‡»å›¾åƒå¼€å§‹æ·»åŠ æç¤º...", elem_id="interaction-info")
                            
                            with gr.Accordion("ğŸ“ é«˜çº§æç¤ºé€‰é¡¹", open=True):
                                text_prompt = gr.Textbox(
                                    label="æ–‡æœ¬æç¤º (Text Prompt)",
                                    placeholder="è¾“å…¥ç‰©ä½“æè¿°ï¼Œä¾‹å¦‚ï¼š'a red car' æˆ– 'ä¸€åªçŒ«'",
                                    lines=1
                                )
                                
                                with gr.Row():
                                    gr.Markdown("ç¤ºä¾‹å¿«é€Ÿå¡«å……ï¼š")
                                    example_text_btn = gr.Button("ğŸ± çŒ«", size="sm")
                                    example_point_btn = gr.Button("ğŸ“ ç¤ºä¾‹ç‚¹", size="sm")
                                
                                with gr.Row(visible=False): # éšè—åŸå§‹åæ ‡è¾“å…¥æ¡†ï¼Œä¿æŒåç«¯é€»è¾‘ä½†å‡å°‘ç•Œé¢å¹²æ‰°
                                    point_prompt = gr.Textbox(label="ç‚¹åæ ‡")
                                    box_prompt = gr.Textbox(label="æ¡†åæ ‡")
                            
                            confidence_threshold = gr.Slider(
                                minimum=0.0, maximum=1.0, value=0.4, step=0.05,
                                label="ğŸ¯ ç½®ä¿¡åº¦é˜ˆå€¼ (Confidence)"
                            )
                            
                            segment_button = gr.Button("ğŸš€ å¼€å§‹åˆ†å‰² (Segment)", variant="primary", size="lg")
                            
                        # å³ä¾§ç»“æœæ 
                        with gr.Column(scale=7):
                            image_output = gr.Image(type="numpy", label="âœ¨ åˆ†å‰²ç»“æœ")
                            image_info = gr.Textbox(label="ğŸ“Š åˆ†ææŠ¥å‘Š", interactive=False, lines=2)
                    
                    # äº‹ä»¶ç»‘å®š
                    
                    # 1. ä¸Šä¼ å›¾ç‰‡æ—¶ä¿å­˜åŸå›¾
                    def store_original_image(img): return img, None # Reset click state
                    image_input.upload(fn=store_original_image, inputs=[image_input], outputs=[original_image_state, click_state])
                    
                    # 2. ç‚¹å‡»å›¾ç‰‡å¤„ç†
                    image_input.select(
                        fn=handle_image_click,
                        inputs=[image_input, original_image_state, interaction_mode, point_prompt, box_prompt, click_state],
                        outputs=[image_input, point_prompt, box_prompt, click_state, interaction_info]
                    )
                    
                    # 3. æ¸…ç©ºæç¤º
                    def clear_prompts(orig_img):
                        if orig_img is None: return None, "", "", None, "è¯·å…ˆä¸Šä¼ å›¾åƒ"
                        return orig_img, "", "", None, "â™»ï¸ æç¤ºå·²æ¸…ç©ºï¼Œå›¾åƒå·²é‡ç½®"
                    clear_prompts_btn.click(
                        fn=clear_prompts,
                        inputs=[original_image_state],
                        outputs=[image_input, point_prompt, box_prompt, click_state, interaction_info]
                    )
                    
                    # 4. åˆ†å‰²æŒ‰é’®
                    segment_button.click(
                        fn=segment_image,
                        inputs=[image_input, text_prompt, confidence_threshold, point_prompt, box_prompt, original_image_state],
                        outputs=[image_output, image_info]
                    )
                    
                    # 5. ç¤ºä¾‹æŒ‰é’®
                    example_text_btn.click(fn=lambda: "a cat", outputs=[text_prompt])
                    example_point_btn.click(fn=lambda: "100,100", outputs=[point_prompt])

                # ================= è§†é¢‘è·Ÿè¸ªæ ‡ç­¾é¡µ =================
                with gr.TabItem("ğŸ¬ è§†é¢‘ç›®æ ‡è·Ÿè¸ª", id="tab_video"):
                    with gr.Row():
                        with gr.Column(scale=1):
                            video_input = gr.Video(label="ğŸ“‚ ä¸Šä¼ è§†é¢‘æ–‡ä»¶")
                            
                            with gr.Group():
                                video_text_prompt = gr.Textbox(
                                    label="ğŸ“ è·Ÿè¸ªç›®æ ‡æè¿°",
                                    placeholder="ä¾‹å¦‚ï¼š'a person running' (ç›®å‰ä»…æ”¯æŒç¬¬ä¸€å¸§æ–‡æœ¬æç¤º)",
                                    lines=2
                                )
                                video_confidence_threshold = gr.Slider(
                                    minimum=0.0, maximum=1.0, value=0.5, step=0.05,
                                    label="ğŸ¯ è·Ÿè¸ªç½®ä¿¡åº¦"
                                )
                            
                            process_button = gr.Button("â–¶ï¸ å¼€å§‹è·Ÿè¸ªå¤„ç†", variant="primary", size="lg")
                            
                        with gr.Column(scale=1):
                            video_output = gr.Video(label="âœ¨ è·Ÿè¸ªç»“æœ")
                            video_info = gr.Textbox(label="ğŸ“Š å¤„ç†æŠ¥å‘Š", interactive=False)
                    
                    process_button.click(
                        fn=process_video,
                        inputs=[video_input, video_text_prompt, video_confidence_threshold],
                        outputs=[video_output, video_info]
                    )
        
        # é¡µè„š
        gr.Markdown("""
        ---
        <div style="text-align: center; color: #718096; font-size: 0.9em;">
            Powered by <strong>SAM3</strong> | 2025 SAM3 Interactive Studio
        </div>
        """)
    
    return demo

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_dir = current_dir / "models"
    if not model_dir.exists():
        print(f"åˆ›å»ºæ¨¡å‹ç›®å½•: {model_dir}")
        model_dir.mkdir(exist_ok=True)
        
    checkpoint_path = model_dir / "sam3.pt"
    bpe_path = current_dir / "assets" / "bpe_simple_vocab_16e6.txt.gz"
    
    if not checkpoint_path.exists() or not bpe_path.exists():
        print("âš ï¸ æ¨¡å‹æ–‡ä»¶ç¼ºå¤±")
        print(f"è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:\n1. {checkpoint_path}\n2. {bpe_path}")
        
        response = input("æ˜¯å¦å°è¯•è‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Ÿ(y/n): ").lower().strip()
        if response == 'y':
            try:
                import download_models
                download_models.main()
            except Exception as e:
                print(f"è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {e}")
                return
        else:
            return
    
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ SAM3 äº¤äº’å¼è§†è§‰å·¥ä½œå°...")
    demo = create_demo()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7890,
        share=False,
        debug=True,
        allowed_paths=[str(current_dir)]
    )

if __name__ == "__main__":
    main()
