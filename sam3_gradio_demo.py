#!/usr/bin/env python3
"""
SAM3 Interactive Vision Studio
åŸºäº SAM3 çš„äº¤äº’å¼å›¾åƒåˆ†å‰²ç³»ç»Ÿ
"""

import os
import sys
import time
import io
import numpy as np
import torch
import gradio as gr
from PIL import Image
import cv2
from pathlib import Path
import json

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥sam3æ¨¡å—
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# å¯¼å…¥SAM3ç›¸å…³æ¨¡å—
try:
    from sam3.model_builder import build_sam3_image_model
    from sam3.model.sam3_image_processor import Sam3Processor
    from sam3.model.data_misc import FindStage
    from sam3.visualization_utils import plot_results
    from sam3.model import box_ops
except ImportError as e:
    print(f"å¯¼å…¥SAM3æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…SAM3ä¾èµ–")
    sys.exit(1)

# å…¨å±€å˜é‡
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# åˆå§‹åŒ–æ¨¡å‹
def initialize_models():
    """åˆå§‹åŒ–SAM3å›¾åƒé¢„æµ‹å™¨"""
    try:
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_dir = current_dir / "models"
        checkpoint_path = model_dir / "sam3.pt"
        bpe_path = current_dir / "assets" / "bpe_simple_vocab_16e6.txt.gz"
        
        if not checkpoint_path.exists():
            print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            print("è¯·ä¸‹è½½SAM3æ¨¡å‹æ–‡ä»¶åˆ°ç›®å½•")
            return None
            
        if not bpe_path.exists():
            print(f"BPEæ–‡ä»¶ä¸å­˜åœ¨: {bpe_path}")
            return None
            
        # åˆå§‹åŒ–å›¾åƒæ¨¡å‹
        image_model = build_sam3_image_model(
            checkpoint_path=str(checkpoint_path),
            bpe_path=str(bpe_path),
            device=DEVICE
        )
        
        # åˆ›å»ºå›¾åƒå¤„ç†å™¨
        image_predictor = Sam3Processor(image_model, device=DEVICE)

        print("æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        return image_predictor
        
    except Exception as e:
        print(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

# å…¨å±€é¢„æµ‹å™¨å®ä¾‹
image_predictor = initialize_models()

def handle_image_click(img, original_img, evt: gr.SelectData, mode, current_points, current_boxes, click_state):
    """å¤„ç†å›¾åƒç‚¹å‡»äº‹ä»¶ï¼Œæä¾›å®æ—¶è§†è§‰åé¦ˆ"""
    if img is None:
        return img, current_points, current_boxes, click_state, "è¯·å…ˆä¸Šä¼ å›¾åƒ"
    
    # å¦‚æœæ²¡æœ‰åŸå§‹å›¾åƒï¼Œå°±ä½¿ç”¨å½“å‰å›¾åƒä½œä¸ºåŸå§‹å›¾åƒ
    if original_img is None:
        original_img = img.copy()
        
    # åŸºäºå½“å‰æ˜¾ç¤ºçš„å›¾åƒè¿›è¡Œç»˜åˆ¶
    vis_img = img.copy()
    
    x, y = evt.index
    x, y = int(x), int(y)
    
    info_msg = ""
    
    if mode == "ğŸ“ ç‚¹æç¤º (Point)":
        new_point = f"{x},{y}"
        if current_points:
            current_points += f";{new_point}"
        else:
            current_points = new_point
            
        # åœ¨å›¾åƒä¸Šç”»çº¢è‰²åœ†ç‚¹
        cv2.circle(vis_img, (x, y), 6, (255, 0, 0), -1) # çº¢è‰²å®å¿ƒåœ†
        cv2.circle(vis_img, (x, y), 6, (255, 255, 255), 1) # ç™½è‰²æè¾¹
        
        info_msg = f"âœ… å·²æ·»åŠ ç‚¹: {new_point}"
        return vis_img, current_points, current_boxes, None, info_msg
        
    elif mode == "ğŸ”² æ¡†æç¤º (Box)":
        if click_state is None:
            # ç¬¬ä¸€æ¬¡ç‚¹å‡» - ç”»èµ·ç‚¹ï¼ˆè“è‰²ï¼‰
            click_state = [x, y]
            cv2.circle(vis_img, (x, y), 6, (0, 0, 255), -1) # è“è‰²å®å¿ƒåœ†
            cv2.circle(vis_img, (x, y), 6, (255, 255, 255), 1) # ç™½è‰²æè¾¹
            info_msg = f"ğŸ”µ å·²è®°å½•èµ·ç‚¹: {x},{y}ï¼Œè¯·ç‚¹å‡»å¯¹è§’ç‚¹å®Œæˆæ¡†é€‰"
            return vis_img, current_points, current_boxes, click_state, info_msg
        else:
            # ç¬¬äºŒæ¬¡ç‚¹å‡» - ç”»æ¡†ï¼ˆç»¿è‰²ï¼‰
            x1, y1 = click_state
            x2, y2 = x, y
            
            xmin = min(x1, x2)
            ymin = min(y1, y2)
            xmax = max(x1, x2)
            ymax = max(y1, y2)
            
            # ç¡®ä¿æ¡†æœ‰å¤§å°
            if xmin == xmax: xmax += 1
            if ymin == ymax: ymax += 1
            
            new_box = f"{xmin},{ymin},{xmax},{ymax}"
            if current_boxes:
                current_boxes += f";{new_box}"
            else:
                current_boxes = new_box
            
            # ç”»ç»¿è‰²çŸ©å½¢æ¡†
            cv2.rectangle(vis_img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 3)
                
            info_msg = f"âœ… å·²æ·»åŠ æ¡†: {new_box}"
            return vis_img, current_points, current_boxes, None, info_msg
    
    return vis_img, current_points, current_boxes, click_state, info_msg

def segment_image(
    input_image,
    text_prompt,
    confidence_threshold,
    point_prompt,
    box_prompt,
    original_image=None,
    progress=gr.Progress()
):
    """å›¾åƒåˆ†å‰²åŠŸèƒ½"""
    # ä¼˜å…ˆä½¿ç”¨åŸå§‹å›¾åƒï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨è¾“å…¥å›¾åƒ
    image_to_process = original_image if original_image is not None else input_image
    
    if image_to_process is None:
        return None, "è¯·ä¸Šä¼ å›¾åƒ"
        
    if not text_prompt and not point_prompt and not box_prompt:
        return None, "è¯·æä¾›è‡³å°‘ä¸€ç§æç¤ºï¼ˆæ–‡æœ¬ã€ç‚¹æˆ–æ¡†ï¼‰"
    
    try:
        if image_predictor is None:
            return None, "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"
            
        start_time = time.time()
        progress(0.1, desc="æ­£åœ¨åŠ è½½å›¾åƒ...")
        
        # è½¬æ¢å›¾åƒæ ¼å¼
        if isinstance(image_to_process, np.ndarray):
            image = Image.fromarray(image_to_process)
        else:
            image = image_to_process
            
        # è®¾ç½®å›¾åƒ
        state = image_predictor.set_image(image)
        progress(0.3, desc="è§£ææç¤ºä¿¡æ¯...")
        
        # å¤„ç†æ–‡æœ¬æç¤º
        if text_prompt:
            state = image_predictor.set_text_prompt(text_prompt, state)
            
        # å¤„ç†ç‚¹æç¤º
        if point_prompt:
            points = []
            for point_str in point_prompt.split(';'):
                if point_str:
                    try:
                        x, y = map(float, point_str.split(','))
                        points.append([x, y])
                    except ValueError:
                        continue
                        
            if points:
                width, height = image.size
                normalized_points = []
                for x, y in points:
                    normalized_points.append([x/width, y/height])
                    
                for point in normalized_points:
                    box_size = min(width, height) * 0.05
                    box_width = box_size / width
                    box_height = box_size / height
                    box = [point[0], point[1], box_width, box_height]
                    state = image_predictor.add_geometric_prompt(box, True, state)
        
        # å¤„ç†æ¡†æç¤º
        if box_prompt:
            boxes = []
            for box_str in box_prompt.split(';'):
                if box_str:
                    try:
                        x1, y1, x2, y2 = map(float, box_str.split(','))
                        boxes.append([x1, y1, x2, y2])
                    except ValueError:
                        continue
                        
            if boxes:
                width, height = image.size
                normalized_boxes = []
                for x1, y1, x2, y2 in boxes:
                    center_x = (x1 + x2) / 2 / width
                    center_y = (y1 + y2) / 2 / height
                    box_width = (x2 - x1) / width
                    box_height = (y2 - y1) / height
                    normalized_boxes.append([center_x, center_y, box_width, box_height])
                    
                for box in normalized_boxes:
                    state = image_predictor.add_geometric_prompt(box, True, state)
        
        # è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼
        state = image_predictor.set_confidence_threshold(confidence_threshold, state)
        
        progress(0.7, desc="æ¨¡å‹æ¨ç†ä¸­...")
        
        # è·å–ç»“æœ
        if "boxes" in state and len(state["boxes"]) > 0:
            # å¯è§†åŒ–ç»“æœ
            import matplotlib.pyplot as plt
            
            # ä½¿ç”¨å®˜æ–¹çš„ plot_results æ¥å£è¿›è¡Œç»˜åˆ¶
            # plot_results å†…éƒ¨ä¼šåˆ›å»º figure å¹¶ç»˜åˆ¶ masks, boxes, scores
            # æ³¨æ„ï¼šå®ƒä¼šæ‰“å°æ‰¾åˆ°çš„å¯¹è±¡æ•°é‡ï¼Œä½†è¿™ä¸å½±å“ Gradio æ˜¾ç¤º
            plot_results(image, state)
            
            # è·å–å½“å‰çš„ figure (ç”± plot_results åˆ›å»º) å¹¶è½¬æ¢ä¸º PIL å›¾åƒ
            buf = io.BytesIO()
            plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
            buf.seek(0)
            result_image = Image.open(buf)
            plt.close() # å…³é—­ figure é‡Šæ”¾å†…å­˜
            
            processing_time = time.time() - start_time
            info = f"âœ¨ å¤„ç†å®Œæˆ | è€—æ—¶: {processing_time:.2f}s | æ£€æµ‹åˆ° {len(state['boxes'])} ä¸ªç›®æ ‡"
            
            return result_image, info
        else:
            return image, "âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•å¯¹è±¡ï¼Œè¯·å°è¯•è°ƒæ•´æç¤ºæˆ–é™ä½ç½®ä¿¡åº¦é˜ˆå€¼"
            
    except Exception as e:
        return None, f"âŒ å¤„ç†å¤±è´¥: {str(e)}"

def create_demo():
    """åˆ›å»ºç¾åŒ–åçš„Gradioæ¼”ç¤ºç•Œé¢"""

    # è‡ªå®šä¹‰CSS
    custom_css = """
    .container { max-width: 1200px; margin: auto; padding-top: 20px; }
    h1 { text-align: center; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; color: #2d3748; margin-bottom: 10px; }
    .description { text-align: center; font-size: 1.1em; color: #4a5568; margin-bottom: 30px; }
    .gr-button-primary { background: linear-gradient(90deg, #4b6cb7 0%, #182848 100%); border: none; }
    .gr-box { border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
    #interaction-info { font-weight: bold; color: #2b6cb0; text-align: center; background-color: #ebf8ff; padding: 10px; border-radius: 5px; border: 1px solid #bee3f8; }

    /* è®©RadioæŒ‰é’®ç»„æ°´å¹³æ’‘æ»¡ */
    .mode-radio .wrap { display: flex; width: 100%; gap: 10px; }
    .mode-radio .wrap label { flex: 1; justify-content: center; text-align: center; }
    """

    theme = gr.themes.Soft(
        primary_hue="blue",
        secondary_hue="slate",
        font=[gr.themes.GoogleFont('Inter'), 'ui-sans-serif', 'system-ui', 'sans-serif']
    )

    with gr.Blocks(theme=theme, css=custom_css, title="SAM3 äº¤äº’å¼è§†è§‰å·¥ä½œå°") as demo:

        with gr.Column(elem_classes="container"):
            gr.Markdown("# ğŸ‘ï¸ SAM3 äº¤äº’å¼è§†è§‰å·¥ä½œå°")
            gr.Markdown("ä¸“æ³¨äº SAM3 çš„äº¤äº’å¼å›¾åƒåˆ†å‰²ä½“éªŒ", elem_classes="description")

            with gr.Row():
                # å·¦ä¾§æ§åˆ¶æ 
                with gr.Column(scale=1):
                    image_input = gr.Image(type="numpy", label="åŸå§‹å›¾åƒ (ç‚¹å‡»è¿›è¡Œäº¤äº’)", elem_id="input_image")

                    # å­˜å‚¨åŸå§‹å›¾åƒçŠ¶æ€
                    original_image_state = gr.State(None)
                    click_state = gr.State(None)

                    with gr.Group():
                        gr.Markdown("### ğŸ® äº¤äº’æ¨¡å¼")
                        # ç¬¬ä¸€è¡Œï¼šæ¨¡å¼é€‰æ‹©
                        interaction_mode = gr.Radio(
                            choices=["ğŸ“ ç‚¹æç¤º (Point)", "ğŸ”² æ¡†æç¤º (Box)"],
                            value="ğŸ“ ç‚¹æç¤º (Point)",
                            label="é€‰æ‹©æ¨¡å¼",
                            show_label=False,
                            elem_classes="mode-radio",
                        )
                        # ç¬¬äºŒè¡Œï¼šæ¸…ç©ºæŒ‰é’®ï¼ˆå…¨å®½ï¼‰
                        with gr.Row():
                            clear_prompts_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæç¤º (Clear Prompts)", size="sm", variant="secondary")

                        interaction_info = gr.Markdown("ğŸ‘† ç‚¹å‡»å›¾åƒå¼€å§‹æ·»åŠ æç¤º...", elem_id="interaction-info")

                    with gr.Accordion("ğŸ“ é«˜çº§æç¤ºé€‰é¡¹", open=True):
                        text_prompt = gr.Textbox(
                            label="æ–‡æœ¬æç¤º (Text Prompt)",
                            placeholder="è¾“å…¥ç‰©ä½“æè¿°ï¼Œä¾‹å¦‚ï¼š'a red car' æˆ– 'ä¸€åªçŒ«'",
                            lines=1
                        )

                        with gr.Row():
                            gr.Markdown("ç¤ºä¾‹å¿«é€Ÿå¡«å……ï¼š")
                            example_text_btn = gr.Button("ğŸ± çŒ«", size="sm")
                            example_point_btn = gr.Button("ğŸ“ ç¤ºä¾‹ç‚¹", size="sm")

                        with gr.Row(visible=False): # éšè—åŸå§‹åæ ‡è¾“å…¥æ¡†ï¼Œä¿æŒåç«¯é€»è¾‘ä½†å‡å°‘ç•Œé¢å¹²æ‰°
                            point_prompt = gr.Textbox(label="ç‚¹åæ ‡")
                            box_prompt = gr.Textbox(label="æ¡†åæ ‡")

                    confidence_threshold = gr.Slider(
                        minimum=0.0, maximum=1.0, value=0.4, step=0.05,
                        label="ğŸ¯ ç½®ä¿¡åº¦é˜ˆå€¼ (Confidence)"
                    )

                    segment_button = gr.Button("ğŸš€ å¼€å§‹åˆ†å‰² (Segment)", variant="primary", size="lg")

                # å³ä¾§ç»“æœæ 
                with gr.Column(scale=1):
                    image_output = gr.Image(type="numpy", label="âœ¨ åˆ†å‰²ç»“æœ")
                    image_info = gr.Textbox(label="ğŸ“Š åˆ†ææŠ¥å‘Š", interactive=False, lines=2)

            # äº‹ä»¶ç»‘å®š

            # 1. ä¸Šä¼ å›¾ç‰‡æ—¶ä¿å­˜åŸå›¾
            def store_original_image(img): return img, None # Reset click state
            image_input.upload(fn=store_original_image, inputs=[image_input], outputs=[original_image_state, click_state])

            # 2. ç‚¹å‡»å›¾ç‰‡å¤„ç†
            image_input.select(
                fn=handle_image_click,
                inputs=[image_input, original_image_state, interaction_mode, point_prompt, box_prompt, click_state],
                outputs=[image_input, point_prompt, box_prompt, click_state, interaction_info]
            )

            # 3. æ¸…ç©ºæç¤º
            def clear_prompts(orig_img):
                if orig_img is None: return None, "", "", None, "è¯·å…ˆä¸Šä¼ å›¾åƒ"
                return orig_img, "", "", None, "â™»ï¸ æç¤ºå·²æ¸…ç©ºï¼Œå›¾åƒå·²é‡ç½®"
            clear_prompts_btn.click(
                fn=clear_prompts,
                inputs=[original_image_state],
                outputs=[image_input, point_prompt, box_prompt, click_state, interaction_info]
            )

            # 4. åˆ†å‰²æŒ‰é’®
            segment_button.click(
                fn=segment_image,
                inputs=[image_input, text_prompt, confidence_threshold, point_prompt, box_prompt, original_image_state],
                outputs=[image_output, image_info]
            )

            # 5. ç¤ºä¾‹æŒ‰é’®
            example_text_btn.click(fn=lambda: "a cat", outputs=[text_prompt])
            example_point_btn.click(fn=lambda: "100,100", outputs=[point_prompt])

        # é¡µè„š
        gr.Markdown("""
        ---
        <div style="text-align: center; color: #718096; font-size: 0.9em;">
            Powered by <strong>SAM3</strong> | 2025 SAM3 Interactive Studio
        </div>
        """)

    return demo

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_dir = current_dir / "models"
    if not model_dir.exists():
        print(f"åˆ›å»ºæ¨¡å‹ç›®å½•: {model_dir}")
        model_dir.mkdir(exist_ok=True)
        
    checkpoint_path = model_dir / "sam3.pt"
    bpe_path = current_dir / "assets" / "bpe_simple_vocab_16e6.txt.gz"
    
    if not checkpoint_path.exists() or not bpe_path.exists():
        print("âš ï¸ æ¨¡å‹æ–‡ä»¶ç¼ºå¤±")
        print(f"è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:\n1. {checkpoint_path}\n2. {bpe_path}")
        
        response = input("æ˜¯å¦å°è¯•è‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Ÿ(y/n): ").lower().strip()
        if response == 'y':
            try:
                import download_models
                download_models.main()
            except Exception as e:
                print(f"è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {e}")
                return
        else:
            return
    
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ SAM3 äº¤äº’å¼è§†è§‰å·¥ä½œå°...")
    demo = create_demo()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7890,
        share=False,
        debug=True,
        allowed_paths=[str(current_dir)]
    )

if __name__ == "__main__":
    main()
